# å¼ºåŒ–å­¦ä¹ 

ä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ ï¼Ÿ

å¼ºåŒ–å­¦ä¹ æ˜¯æ™ºèƒ½ä½“(agent)åœ¨ç¯å¢ƒ(environment)ä¸­é‡‡å–åŠ¨ä½œ(action)ä»¥**æœ€å¤§åŒ–å¥–åŠ±(reward)çš„ç­–ç•¥(policy)**

å¼ºåŒ–å­¦ä¹ çš„ç»ˆæç›®æ ‡ï¼šæ±‚è§£æœ€ä¼˜ç­–ç•¥

![image-20231220194010506](Reinforcement learning.assets/image-20231220194010506.png)

## åŸºæœ¬æ¦‚å¿µ

### æ™ºèƒ½ä½“(agent)

å¼ºåŒ–å­¦ä¹ çš„å¯¹è±¡

### çŠ¶æ€(State)

çŠ¶æ€æ˜¯agentä¸ç¯å¢ƒäº¤äº’è¿‡ç¨‹çš„æ¯ä¸€ä¸ªé˜¶æ®µ

> The status of the agent with respect to the environment

æˆ‘ä»¬ç”¨ä¾‹å­æ›´åŠ å½¢è±¡åœ°è¯´æ˜ä»€ä¹ˆæ˜¯çŠ¶æ€

ä»¥grid-world exampleä¸ºä¾‹ï¼Œagentå¯ä»¥åœ¨è¯¥ç½‘æ ¼ä¸–ç•Œä¸­ç§»åŠ¨åˆ°æ¯ä¸€ä¸ªæ ¼å­ï¼Œæ¯ä¸ªç½‘æ ¼å°±æ˜¯ä¸€ä¸ªçŠ¶æ€

![image-20231220193653967](Reinforcement learning.assets/image-20231220193653967.png)



### çŠ¶æ€ç©ºé—´(State space)

the set of all states 

åŒæ ·ä»¥grid-worldä¸ºä¾‹ï¼Œ$S = \{{s_i}\}_{i=1}^9$

### åŠ¨ä½œ(Action)

åœ¨**æŸä¸ªçŠ¶æ€**ï¼Œagentèƒ½å¤Ÿé‡‡å–çš„èƒ½å¤Ÿå½±å“ç¯å¢ƒçŠ¶æ€çš„è¡Œä¸º

- åŠ¨ä½œä¾èµ–äºçŠ¶æ€ï¼Œå³ä¸åŒçŠ¶æ€èƒ½å¤Ÿé‡‡å–çš„åŠ¨ä½œä¹Ÿä¸åŒ

- åŒæ ·ä»¥grid-worldä¸ºä¾‹ï¼Œå¯¹äºæ¯ä¸ªçŠ¶æ€éƒ½æœ‰å¦‚ä¸‹å¯èƒ½çš„ä¸œåŠ¨ä½œï¼š

  *â€¢* *a*1: move upwards;

  *â€¢* *a*2: move rightwards;

  *â€¢* *a*3: move downwards;

  *â€¢* *a*4: move leftwards;

  *â€¢* *a*5: stay unchanged;

### åŠ¨ä½œç©ºé—´(Action space **of a state**)

the set of all possible actions **of a state**

- åŒæ ·ä»¥grid-worldä¸ºä¾‹ï¼Œ$A(s_i) = \{{a_i}\}^{5}_{i=1}.$

### çŠ¶æ€è½¬æ¢( state transition)

æ™ºèƒ½ä½“é‡‡å–ä¸€ä¸ªåŠ¨ä½œä¹‹åï¼Œä»ä¸€ä¸ªçŠ¶æ€åˆ°è¾¾å¦ä¸€ä¸ªçŠ¶æ€çš„è¿‡ç¨‹

- æˆ‘ä»¬ä¸€èˆ¬ä½¿ç”¨æ¡ä»¶æ¦‚ç‡æè¿°çŠ¶æ€è½¬æ¢

- å¦‚ä¸‹å›¾ï¼Œ$p(s_2|s_1,a_2)=1$è¡¨ç¤ºagenté‡‡å–a2åŠ¨ä½œä¹‹åä»çŠ¶æ€s1åˆ°çŠ¶æ€s2çš„å¯èƒ½æ€§ä¸º1

  åœ¨ç¡®å®šæ€§æƒ…æ™¯ä¸­ï¼ŒçŠ¶æ€è½¬æ¢çš„æ¡ä»¶æ¦‚ç‡ä¸º1æˆ–0

  åœ¨éšæœºæ€§æƒ…å¢ƒä¸­ï¼ŒçŠ¶æ€è½¬æ¢çš„æ¡ä»¶æ¦‚ç‡ä¸º0åˆ°1ä¹‹é—´çš„ä»»ä¸€å€¼ã€‚

  ![image-20231220200029972](Reinforcement learning.assets/image-20231220200029972.png)

### ç­–ç•¥(Policy)

*Policy* tells the agent what actions to take **at a state.** 

- ç­–ç•¥æŒ‡çš„æ˜¯æ¯ä¸ªçŠ¶æ€çš„ç­–ç•¥ï¼Œè€Œä¸æ˜¯å…¨å±€çš„ç­–ç•¥ã€‚

- ä¸¾ä¸ªä¾‹å­ï¼šåœ¨grid-worldä¾‹å­ä¸­ï¼ŒpolicyæŒ‡æŒ¥ä¹ä¸ªæ ¼å­åˆ†åˆ«åº”è¯¥é‡‡å–ä»€ä¹ˆåŠ¨ä½œï¼Œè€Œä¸æ˜¯ä»æŸä¸€å¼€å§‹çŠ¶æ€åˆ°ç»ˆæ­¢çŠ¶æ€çš„ç‰¹å®šè·¯çº¿ï¼ˆå³ä¸‹é¢æåˆ°çš„Pathï¼‰ç»è¿‡çš„çŠ¶æ€ï¼ˆç‰¹å®šè·¯çº¿ç»è¿‡çš„çŠ¶æ€ä¸€å®šå°‘äºæˆ–ç­‰äº9ä¸ªçŠ¶æ€ï¼‰

  ![image-20231220200840060](Reinforcement learning.assets/image-20231220200840060.png)

- Pathï¼šä»æŸä¸€å¼€å§‹çŠ¶æ€åˆ°ç»ˆæ­¢çŠ¶æ€ç»è¿‡çš„çŠ¶æ€é›†åˆ

- æˆ‘ä»¬é€šå¸¸ä½¿ç”¨æ¡ä»¶æ¦‚ç‡æè¿°policy

  > åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ç¬¦å·$\pi$è¡¨ç¤ºPolicy

  **ç¡®å®šæ€§æƒ…æ™¯ï¼š**

  $\piï¼ˆa_2|s_1)=1$è¡¨ç¤ºåœ¨çŠ¶æ€s1é‡‡å–åŠ¨ä½œa2çš„å¯èƒ½æ€§ä¸º1

  ![image-20231220201112111](Reinforcement learning.assets/image-20231220201112111.png)

  **éšæœºæ€§æƒ…æ™¯ï¼š**

  $\piï¼ˆa_2|s_1)=0.5$è¡¨ç¤ºåœ¨çŠ¶æ€s1é‡‡å–åŠ¨ä½œa2çš„å¯èƒ½æ€§ä¸º0.5

  ![image-20231220201139571](Reinforcement learning.assets/image-20231220201139571.png)

### å¥–èµ(Reward)

a **real number** we get after taking an action. 

- A real numberæŒ‡ï¼šå›æŠ¥æ˜¯æ ‡é‡
- å¥–èµæ˜¯**æŸä¸ªçŠ¶æ€**é‡‡å–**æŸä¸ªåŠ¨ä½œ**åçš„å¥–èµï¼Œè€Œä¸æ˜¯å…¨å±€çš„å¥–èµ
- A **positive** reward represents **encouragement** to take such actions.

- A **negative** reward represents **punishment** to take such actions.

- Zero rewardè¡¨ç¤ºæ²¡æœ‰æƒ©ç½šï¼Œè¿™ä¹Ÿç®—æ˜¯ä¸€ç§encouragement

- å½“ç„¶ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨negative rewardè¡¨ç¤ºencouragementï¼Œä½¿ç”¨positive rewardè¡¨ç¤ºpunishmentï¼Œæ­¤æ—¶æˆ‘ä»¬çš„æœ€ä¼˜ç­–ç•¥å°±ä»maxmize total rewards å˜æˆ minimize total rewards

- **å¥–èµä¾èµ–äºå½“å‰çš„çŠ¶æ€å’ŒåŠ¨ä½œï¼Œè€Œä¸æ˜¯ä¸‹ä¸€ä¸ªçŠ¶æ€**

- Reward can be interpreted as a **human-machine interface**, with which we can guide the agent to behave as what we expect.

- å¥–èµçš„å€¼ä¹Ÿåˆ†ä¸ºç¡®å®šæ€§æƒ…æ™¯å’Œéšæœºæ€§æƒ…æ™¯ä¸¤ç§æƒ…å†µ

  - ç¡®å®šæ€§æƒ…æ™¯ï¼š

    $p(r = -1|s_1, a_1) = 1$è¡¨ç¤ºçŠ¶æ€s1é‡‡å–a1åŠ¨ä½œå¾—åˆ°å¥–èµä¸º-1çš„å¯èƒ½æ€§ä¸º1

    ![image-20231220202444978](Reinforcement learning.assets/image-20231220202444978.png)

  - éšæœºæ€§æƒ…æ™¯

    åŒæ ·ä»¥ä¸Šå›¾ä¸ºä¾‹ï¼Œ$p(r = -1|s_1, a_1) = 0.5$è¡¨ç¤ºçŠ¶æ€s1é‡‡å–a1åŠ¨ä½œå¾—åˆ°å¥–èµä¸º-1çš„å¯èƒ½æ€§ä¸º0.5ï¼Œ$p(r = 1|s_1, a_1) = 0.5$è¡¨ç¤ºçŠ¶æ€s1é‡‡å–a1åŠ¨ä½œå¾—åˆ°å¥–èµä¸º1çš„å¯èƒ½æ€§ä¸º0.5

    æ‰€ä»¥çŠ¶æ€s1é‡‡å–åŠ¨ä½œa1æœ‰0.5çš„æ¦‚ç‡å¾—åˆ°å¥–èµ1ï¼Œæœ‰0.5çš„æ¦‚ç‡å¾—åˆ°å¥–èµ-1

  

### è½¨è¿¹Trajectory & å›æŠ¥Return

![image-20231220203054949](Reinforcement learning.assets/image-20231220203054949.png)

Trajectoryå¯ä»¥æ— é™é•¿ï¼Œå½“Trajectoryæ— é™é•¿ï¼Œå¾—åˆ°çš„Returnæœ‰å¯èƒ½æ˜¯æ— ç©·

<u>**Return** could be used to evaluate whether a policy is good or not</u>

#### Discounted return

æˆ‘ä»¬åœ¨å‰é¢æåˆ°â€œTrajectoryå¯ä»¥æ— é™é•¿ï¼Œå½“Trajectoryæ— é™é•¿ï¼Œå¾—åˆ°çš„Returnæœ‰å¯èƒ½æ˜¯æ— ç©·â€ï¼Œä¾‹å¦‚ä¸‹å›¾ï¼š

åˆ°è¾¾ç»ˆæ­¢çŠ¶æ€s9åï¼Œç­–ç•¥é€‰æ‹©åŸåœ°ä¸åŠ¨ï¼Œé‚£ä¹ˆå°±ä¼šä¸€ç›´å¾ªç¯åœ¨s9çŠ¶æ€

![image-20231220203954506](Reinforcement learning.assets/image-20231220203954506.png)

ä¸ºäº†é¿å…Returnä¸ºæ— ç©·çš„æƒ…å†µï¼Œæˆ‘ä»¬å¼•å…¥æŠ˜æ‰£å› å­(discount rate)

$\gamma âˆˆ [0, 1)$

å¼•å…¥æŠ˜æ‰£å› å­åï¼ŒåŸæ¥æ— ç©·çš„Returnå˜ä¸ºæœ‰é™å€¼ï¼Œå¯ä»¥é€šè¿‡æ— ç©·çº§æ•°è®¡ç®—å¾—åˆ°

é™¤æ­¤ä¹‹å¤–ï¼ŒæŠ˜æ‰£å› å­è¿˜æœ‰æ›´ç¥å¥‡çš„ç”¨é€”ï¼šå®ƒæ˜¯ä¸€ä¸ªèµ‹äºˆç°åœ¨å€¼å’Œæœªæ¥å€¼çš„æƒé‡

- $\gamma$è¶Šæ¥è¿‘0ï¼Œè¡¨ç¤ºè¶Šé‡è§†å½“ä¸‹å€¼
- $\gamma$è¶Šæ¥è¿‘1ï¼Œè¡¨ç¤ºè¶Šé‡è§†æœªæ¥å€¼

![image-20231220204244296](Reinforcement learning.assets/image-20231220204244296.png)

### Episode

> When interacting with the environment following a policy, the agent may stop at some *terminal states*. The resulting trajectory is called an *episode* (or a trial)

ä¸­æ–‡å¥½åƒæ²¡æœ‰å¾ˆå¥½çš„è§£é‡Š

æ¸¸æˆé‡Œé¢çš„å…³å¡å’Œå‰§é›†é‡Œé¢æ¯ä¸€é›†e1ã€e2çš„eéƒ½ç”¨episodeè¡¨ç¤º

åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼ŒEpisodeæŒ‡agentåœ¨ç¯å¢ƒé‡Œé¢æ‰§è¡ŒæŸä¸ªç­–ç•¥ä»å¼€å§‹åˆ°ç»“æŸè¿™ä¸€è¿‡ç¨‹ã€‚

Episodeæ˜¯æœ‰é™é•¿çš„Trajectory

**æœ‰é™è¿‡ç¨‹çš„ä»»åŠ¡å«åšepisodic tasks**

Tasks with episodes are called ***episodic tasks*.**

**æ— é™è¿‡ç¨‹çš„ä»»åŠ¡å«åšcontinuing tasks**

Some tasks may have no terminal states, meaning the interaction with the environment will never end. Such tasks are called ***continuing tasks***.

**æˆ‘ä»¬å¯ä»¥å°†episodic tasksè½¬æ¢ä¸ºcontinuing tasksï¼š**

> In fact, we can treat episodic and continuing tasks in a unified mathematical way by converting episodic tasks to continuing tasks.
>
> - Option 1: Treat the target state as a special absorbing state. Once the agent reaches an absorbing state, it will never leave. The consequent rewards *r* = 0.
> - Option 2: Treat the target state as a normal state with a policy. The agent can still leave the target state and gain *r* = +1 when entering the target state.
>
> We consider option 2 in this course so that we donâ€™t need to distinguish the target state from the others and can treat it as a normal state.

æœ‰ä¸¤ç§è½¬æ¢æ–¹æ³•ï¼š

1. å°†ç›®æ ‡çŠ¶æ€è§†ä½œâ€œç­–ç•¥ä¸ºï¼šé‡‡å–åœç•™åœ¨åŸåœ°çš„åŠ¨ä½œï¼Œå¹¶ä¸”è¯¥ç»ˆæ­¢çŠ¶æ€é‡‡å–è¯¥åŠ¨ä½œåå¥–èµä¸º0â€çš„çŠ¶æ€
2. å°†ç›®æ ‡çŠ¶æ€è§†ä½œæ™®é€šçŠ¶æ€ï¼Œagentåˆ°è¾¾ç›®æ ‡çŠ¶æ€ä¸ä¼šåœä¸‹ï¼Œ è€Œæ˜¯ç»§ç»­èµ°ï¼Œç›®æ ‡çŠ¶æ€ä¸å…¶ä»–çŠ¶æ€å”¯ä¸€ä¸åŒä¹‹å¤„ï¼šç›®æ ‡çŠ¶æ€çš„å¥–èµä¸º+1



### é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹(Markov decision process, MDP)

é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (Markov decision process, MDP) æè¿°äº†å¼ºåŒ–å­¦ä¹ çš„ç¯å¢ƒè§„å¾‹ï¼Œå‡ ä¹æ‰€æœ‰çš„å¼ºåŒ–å­¦ä¹ é—®é¢˜éƒ½å¯ä»¥å½¢å¼åŒ–ä¸ºMDPs

MDPæ¡†æ¶åŒ…æ‹¬äº†ä¸‰ä¸ªéƒ¨åˆ†ï¼Œä¹Ÿå°±æ˜¯åå­—æ‰€æåˆ°çš„Markov process&decison

#### é©¬å°”ç§‘å¤«æ€§è´¨(Markov)

é©¬å°”å¯å¤«æ€§è´¨ï¼š

$ğ‘ƒ(ğ‘†_{ğ‘¡+1}|ğ‘†1, ğ‘†2, ğ‘†3, â€¦ , ğ‘†ğ‘¡) = ğ‘ƒ(ğ‘†_{ğ‘¡+1}|ğ‘†ğ‘¡) $

â¢ç»™å®šå½“å‰çŠ¶æ€ğ‘†ğ‘¡ ï¼Œæœªæ¥$ğ‘†_{ğ‘¡+1}$ä¸å†å²$ğ‘†_1, ğ‘†_2, ğ‘†_3,â€¦$æ— å…³

â¢$ğ‘†_{ğ‘¡+1}$å–å†³äº$ğ‘†_ğ‘¡$ï¼Œä¸ç”¨è€ƒè™‘å†å²çŠ¶æ€

é©¬å°”ç§‘å¤«è¿‡ç¨‹å°±æ˜¯å…·æœ‰é©¬å°”ç§‘å¤«æ€§è´¨çš„ç¦»æ•£éšæœºè¿‡ç¨‹

#### è¿‡ç¨‹(process)

![image-20231222154459364](Reinforcement learning.assets/image-20231222154459364.png)

è¿‡ç¨‹æŒ‡çš„å°±æ˜¯**éšæœºè¿‡ç¨‹**ï¼ŒåŒ…æ‹¬è¿‡ç¨‹è®°è½½çš„é›†åˆå’Œéšæœºæ¦‚ç‡

é›†åˆåŒ…æ‹¬ï¼š

- æœ‰é™ä¸ªçŠ¶æ€é›†åˆ
- åŠ¨ä½œé›†åˆ
- å¥–èµé›†åˆ

éšæœºæ¦‚ç‡åŒ…æ‹¬ï¼š

1. å½“å‰çŠ¶æ€sé‡‡å–åŠ¨ä½œaåˆ°è¾¾çŠ¶æ€s'çš„æ¦‚ç‡$p(s'|s,a)$
2. å½“å‰çŠ¶æ€sé‡‡å–åŠ¨ä½œaè·å¾—å¥–èµrçš„æ¦‚ç‡$p(r|s,a)$

å°†åˆ†ç‚¹1çš„æ¦‚ç‡$p(s'|s,a)$æ±‡æ€»å³ä¸º**çŠ¶æ€è½¬ç§»çŸ©é˜µ**

![image-20231222155342467](Reinforcement learning.assets/image-20231222155342467.png)

#### å†³ç­–(decision)

å†³ç­–æŒ‡çš„å°±æ˜¯ç­–ç•¥(policy)ï¼ŒæŒ‡æŒ¥agentåœ¨æ¯ä¸ªçŠ¶æ€åº”è¯¥é‡‡å–ä»€ä¹ˆåŠ¨ä½œ

#### è¡¥å……

##### é©¬å°”å¯å¤«è¿‡ç¨‹

é©¬å°”å¯å¤«è¿‡ç¨‹ (Markov Process)æ˜¯æ»¡è¶³é©¬å°”å¯å¤«æ€§è´¨

çš„ç¦»æ•£éšæœºè¿‡ç¨‹ï¼Œåˆç§°**é©¬å°”å¯å¤«é“¾ (Markov Chain)**

â¢ æœ‰é™ä¸ªçŠ¶æ€é›†åˆğ‘†

â¢ çŠ¶æ€è½¬ç§»çŸ©é˜µP

##### é©¬å°”å¯å¤«å¥–èµè¿‡ç¨‹

é©¬å°”å¯å¤«å¥–èµè¿‡ç¨‹ï¼šé©¬å°”å¯å¤«é“¾+å¥–èµ

â€¢ æœ‰é™ä¸ªçŠ¶æ€é›†åˆğ‘†

â€¢ çŠ¶æ€è½¬ç§»çŸ©é˜µğ‘ƒ

â€¢ å¥–èµå‡½æ•°$ğ‘…_ğ‘  = ğ”¼[ğ‘…_{ğ‘¡+1}|ğ‘†_ğ‘¡ = ğ‘ ]$

â€¢ æŠ˜æ‰£å› å­ğ›¾ âˆˆ [0,1]



## è´å°”æ›¼æ–¹ç¨‹

ä¸ºäº†æ›´å¥½åœ°è¡¨è¿°â€œå›æŠ¥â€ï¼Œæˆ‘ä»¬ç”¨éšæœºå˜é‡$G_t$è¡¨ç¤ºçŠ¶æ€$S_t$å‡ºå‘å¾—åˆ°çš„æŸæ¡Trajectoryæ‰€å¾—åˆ°çš„å›æŠ¥(è¿™é‡Œå®é™…ä¸ºdiscounted return)

![image-20231222155835447](Reinforcement learning.assets/image-20231222155835447.png)

### çŠ¶æ€å€¼å‡½æ•°(State value function)/çŠ¶æ€å€¼

**ä»çŠ¶æ€$ğ‘†_ğ‘¡$å¼€å§‹ï¼Œ é‡‡å–ç­–ç•¥ğœ‹ï¼Œè·å¾—çš„å›æŠ¥$ğº_ğ‘¡$çš„æœŸæœ›**  
$$
ğ‘‰_ğœ‹(ğ‘ ) = ğ”¼_ğœ‹[ğº_ğ‘¡|ğ‘†_ğ‘¡=ğ‘ ]=ğ”¼_ğœ‹[ğ‘…_{ğ‘¡+1} + ğ›¾ğ‘…_{ğ‘¡+2} + ğ›¾^2ğ‘…_{ğ‘¡+3} + â‹¯ |ğ‘†_ğ‘¡ = ğ‘ 
$$
æœ‰å‡ ç‚¹éœ€è¦æ³¨æ„ï¼š

1. çŠ¶æ€å€¼å‡½æ•°æ˜¯é’ˆå¯¹çŠ¶æ€så’Œç­–ç•¥$\pi$çš„å‡½æ•°ã€‚ç›¸åŒçŠ¶æ€ä¸‹é‡‡å–ä¸åŒç­–ç•¥$\pi$æ‰€å¾—åˆ°çš„çŠ¶æ€å€¼ä¸åŒï¼Œç›¸åŒç­–ç•¥ä¸åŒçŠ¶æ€sæ‰€å¾—åˆ°çš„çŠ¶æ€å€¼ä¹Ÿä¸åŒ

2. **çŠ¶æ€å€¼$ğ‘‰_ğœ‹(ğ‘ )$å’Œå›æŠ¥$G_t$çš„åŒºåˆ«**

   çŠ¶æ€å€¼å‡½æ•°$ğ‘‰_ğœ‹(ğ‘ )$æ˜¯ä»çŠ¶æ€$ğ‘†_ğ‘¡$å¼€å§‹èƒ½è·å¾—çš„æ‰€æœ‰å›æŠ¥$G_t$çš„å¹³å‡å€¼ï¼ˆå³æ‰€æœ‰å¯èƒ½è½¨è¿¹ï¼‰

   å¦‚æœè½¨è¿¹åªæœ‰ä¸€æ¡ï¼Œå³æ‰€æœ‰æƒ…å†µéƒ½æ˜¯ç¡®å®šçš„ï¼Œå³$Ï€(a|s), p(r|s,a), p(s_0|s,a) $ éƒ½æ˜¯â€œç¡®å®šçš„â€ï¼ˆè¿™é‡Œçš„ç¡®å®šæ„ä¸ºï¼Œæ¦‚ç‡ä¸º1æˆ–0ï¼‰ï¼Œé‚£ä¹ˆçŠ¶æ€å€¼å‡½æ•°$ğ‘‰_ğœ‹(ğ‘ )$ä¸å›æŠ¥$G_t$ç›¸åŒ

3. çŠ¶æ€å€¼å‡½æ•°å¯ç”¨äºè¯„ä»·å½“å‰ç­–ç•¥çš„å¥½åï¼ŒçŠ¶æ€å€¼è¶Šå¤§ï¼Œè¯´æ˜ç­–ç•¥è¶Šå¥½ï¼Œå› ä¸ºæ‰€èƒ½å¾—åˆ°çš„å›æŠ¥æœŸæœ›è¶Šå¤§

### åŠ¨ä½œå€¼å‡½æ•°(Action-value function)

**ä»çŠ¶æ€ğ‘ å¹¶æ‰§è¡ŒåŠ¨ä½œğ‘å¼€å§‹ï¼Œé‡‡å–ç­–ç•¥ğœ‹ï¼Œè·å¾—çš„å›æŠ¥çš„æœŸæœ›**  
$$
ğ‘_ğœ‹(ğ‘ ,ğ‘) = ğ”¼_ğœ‹[ğº_ğ‘¡|ğ‘†_ğ‘¡ = ğ‘ , ğ´_ğ‘¡ = ğ‘]= ğ”¼_ğœ‹[ğ‘…_{ğ‘¡+1} + ğ›¾ğ‘…_{ğ‘¡+2} + ğ›¾^2ğ‘…_{ğ‘¡+3} + â‹¯ ğ‘†_ğ‘¡ = ğ‘ , ğ´_ğ‘¡ = ğ‘
$$
æœ‰å‡ ç‚¹éœ€è¦æ³¨æ„ï¼š

1. åŠ¨ä½œå€¼å‡½æ•°æ˜¯é’ˆå¯¹çŠ¶æ€sï¼ŒåŠ¨ä½œaå’Œ**ç­–ç•¥$\pi$**çš„å‡½æ•°ã€‚

### çŠ¶æ€å€¼å‡½æ•°å’ŒåŠ¨ä½œå€¼å‡½æ•°çš„å…³ç³»

$$

\begin{aligned} 
v_Ï€(s) &= \Sigma_aÏ€(a|s)q_Ï€(s,a) (2)\\
q_Ï€(s,a) &= \Sigma_rp(r|s,a)r + \gamma\Sigma_{s'}p(s'|s,a)v_Ï€(s') (4)
\end{aligned} 
$$



(2)å¼å‘Šè¯‰æˆ‘ä»¬ï¼šå¯ä»¥ä»åŠ¨ä½œå€¼å‡½æ•°å¾—åˆ°çŠ¶æ€å€¼å‡½æ•°

(4)å¼å‘Šè¯‰æˆ‘ä»¬ï¼šå¯ä»¥ä»çŠ¶æ€å€¼å‡½æ•°å¾—åˆ°åŠ¨ä½œå€¼å‡½æ•°

### è´å°”æ›¼æ–¹ç¨‹

![image-20231223204635761](Reinforcement learning.assets/image-20231223204635761.png)

#### çŠ¶æ€å€¼å‡½æ•°æ¨å¯¼

$G_t$å¯ä»¥å†™æˆ
$$
\begin{aligned} 
G_t &= ğ‘…_{ğ‘¡+1} + ğ›¾ğ‘…_{ğ‘¡+2} + ğ›¾^2ğ‘…_{ğ‘¡+3} + â‹¯\\
	&=ğ‘…_{ğ‘¡+1} + ğ›¾(ğ‘…_{ğ‘¡+2} + ğ›¾ğ‘…_{ğ‘¡+3} + â‹¯)\\
	&=ğ‘…_{ğ‘¡+1} + ğ›¾G_{t+1}
\end{aligned}
$$
çŠ¶æ€å€¼å‡½æ•°å¯ä»¥å†™æˆ
$$
\begin{aligned} 
V_Ï€(s) &= E[G_t|S_t = s]\\
&= E[R_{t+1} + Î³G_{t+1}|S_t = s]\\
&= E[R_{t+1}|S_t = s] + Î³E[G_{t+1}|S_t = s]
\end{aligned}
$$
ç„¶ååˆ†åˆ«è®¡ç®—$E[R_{t+1}|S_t = s]$å’Œ$E[G_{t+1}|S_t = s]$

##### è®¡ç®—$E[R_{t+1}|S_t = s]$

æ³¨ï¼šé‡‡å–åŠ¨ä½œaä¹‹åæœ‰ä¸åŒæ¦‚ç‡å¾—åˆ°å¯¹åº”ä¸åŒå€¼$R_{t+1}$ï¼Œä½†æˆ‘ä»¬åªæƒ³è¦å¾—åˆ°é‡‡å–åŠ¨ä½œaå¾—åˆ°çš„$R_{t+1}$ï¼Œæ‰€ä»¥å¯¹é‡‡å–åŠ¨ä½œaå¾—åˆ°çš„æ‰€æœ‰å¯èƒ½çš„$R_{t+1}$æ±‚æœŸæœ›

![image-20231223161309928](Reinforcement learning.assets/image-20231223161309928.png)

##### è®¡ç®—$E[G_{t+1}|S_t = s]$

æ³¨ï¼šç”±äºé©¬å°”å¯å¤«æ€§è´¨ï¼Œ$E[G_{t+1}|S_t = s,S_{t+1} = s']=E[G_{t+1}|S_{t+1} = s']$

![image-20231223161322542](Reinforcement learning.assets/image-20231223161322542.png)

**æŠŠä¸¤æ­¥è®¡ç®—ç»“æœç»¼åˆèµ·æ¥**

![image-20231223163717159](Reinforcement learning.assets/image-20231223163717159.png)

![image-20231223163945009](Reinforcement learning.assets/image-20231223163945009.png)

**æŠŠè¿™ä¸ªç»“æœæ•´ç†ä¸€ä¸‹ï¼š**

è¿™ä¸ªç»“æœè¡¨ç¤ºçŠ¶æ€sé‡‡å–åŠ¨ä½œaæ‰€èƒ½å¾—åˆ°çš„returnçš„æœŸæœ›
$$
\begin{aligned} 
\Sigma_rp(r|s,a)r&=ğ”¼_ğœ‹(R_{t+1}|S_t=s,A_t=a)\\
\Sigma_{s'}p(s'|s,a)v_\pi(s')&=ğ”¼_ğœ‹(G_{t+1}|S_t=s,A_t=a)\\
ğ”¼_ğœ‹(R_{t+1}|S_t=s,A_t=a) + ğ›¾ğ”¼_ğœ‹(G_{t+1}|S_t=s,A_t=a)&= ğ”¼_ğœ‹(R_{t+1}+ğ›¾G_{t+1}|S_t=s,A_t=a) \\
\Sigma_a\pi(a|s)ğ”¼_ğœ‹(R_{t+1}+ğ›¾G_{t+1}|S_t=s,A_t=a) &=ğ”¼_ğœ‹[ğ‘…_{ğ‘¡+1} + ğ›¾ğ‘‰_ğœ‹(ğ‘†_{ğ‘¡+1}) |ğ‘†_ğ‘¡ = ğ‘ ]
\end{aligned}
$$
![image-20231223164754096](Reinforcement learning.assets/image-20231223164754096.png)

æˆ‘ä»¬å°±èƒ½å¾—åˆ°
$$
\begin{aligned} 
ğ‘‰_ğœ‹(ğ‘ ) = ğ”¼_ğœ‹[ğ‘…_{ğ‘¡+1} + ğ›¾ğ‘‰_ğœ‹(ğ‘†_{ğ‘¡+1}) |ğ‘†_ğ‘¡ = ğ‘ ]
\end{aligned}
$$
æŒ–ä¸ªå‘ï¼Œæ„Ÿè§‰è¿™ä¸ªæ¨å¯¼è¿˜æ˜¯ä¸å¤ªå¯¹ï¼Œå…·ä½“è§

[MDPè´å°”æ›¼æ–¹ç¨‹çš„è¯¦ç»†æ¨å¯¼ - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/422749374)

#### åŠ¨ä½œå€¼å‡½æ•°æ¨å¯¼

æˆ‘ä»¬ç”±çŠ¶æ€å€¼å‡½æ•°æ¨å¯¼å¯çŸ¥ï¼š
$$
\begin{aligned}
ğ‘‰_ğœ‹(ğ‘ ) = ğ”¼_ğœ‹[ğº_ğ‘¡|ğ‘†_ğ‘¡=ğ‘ ]
&=\Sigma_a\pi(a|s)ğ”¼_ğœ‹[ğº_ğ‘¡|ğ‘†_ğ‘¡ = ğ‘ , ğ´_ğ‘¡ =ğ‘]\\
&=\Sigma_a\pi(a|s)E(R_{t+1}+ğ›¾G_{t+1}|S_t=s,A_t=a) \\
&= ğ”¼_ğœ‹[ğ‘…_{ğ‘¡+1} + ğ›¾ğ‘‰_ğœ‹(ğ‘†_{ğ‘¡+1}) |ğ‘†_ğ‘¡ = ğ‘ ]\\
\end{aligned}
$$
ç”±åŠ¨ä½œå€¼å‡½æ•°å®šä¹‰å¯çŸ¥ï¼š
$$
ğ‘_ğœ‹(ğ‘ ,ğ‘) = ğ”¼_ğœ‹[ğº_ğ‘¡|ğ‘†_ğ‘¡ = ğ‘ , ğ´_ğ‘¡ = ğ‘]
$$
æ‰€ä»¥å¯ä»¥å¾—åˆ°ï¼š
$$
ğ‘‰_ğœ‹(ğ‘ ) =\Sigma_a\pi(a|s)ğ‘_ğœ‹(ğ‘ ,ğ‘)
$$
æˆ‘ä»¬ä¹Ÿèƒ½å¾—åˆ°ï¼š

åŠ¨ä½œå€¼å‡½æ•°$ğ‘_ğœ‹(ğ‘ ,ğ‘)$ç­‰äºè¿™ä¸ªå¼å­

![image-20231223164754096](Reinforcement learning.assets/image-20231223164754096.png)
